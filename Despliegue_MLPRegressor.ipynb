{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Instalaci√≥n\n",
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y00xs35Wxsag",
        "outputId": "64cb44c1-a2b6-41bf-cc29-0e15d75d8847"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.40.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "OQyjUh21xGzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcc00a0-98d7-486d-8b73-c2f68d0d6534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "st.set_page_config(page_title=\"Predicci√≥n APGAR2\", page_icon=\"üçº\")\n",
        "\n",
        "st.title(\"Predicci√≥n de APGAR2\")\n",
        "st.write(\"Sube un archivo CSV con los datos para predecir APGAR2.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class APGARCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.sexo_moda_ = None\n",
        "        self.imputer_ = None\n",
        "        self.valid_indices_ = None\n",
        "\n",
        "    def _normalizar_tipo_parto(self, df):\n",
        "        \"\"\"Normaliza todos los tipos de TIPO PARTO a versiones sin tildes\"\"\"\n",
        "        if 'TIPO PARTO' in df.columns:\n",
        "            df['TIPO PARTO'] = df['TIPO PARTO'].replace({\n",
        "                'ESPONTANEO': 'ESPONTANEO',\n",
        "                'ESPONT√ÉNEO': 'ESPONTANEO',\n",
        "                'ESPONT√ÅNEO': 'ESPONTANEO',\n",
        "                'CESAREA': 'CESAREA',\n",
        "                'CES√ÉREA': 'CESAREA',\n",
        "                'CES√ÅREA': 'CESAREA',\n",
        "                'INSTRUMENTADO': 'INSTRUMENTADO'\n",
        "            })\n",
        "        return df\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # CLAVE: Resetear √≠ndices desde el inicio para evitar problemas\n",
        "        df_clean = X.copy().reset_index(drop=True)\n",
        "        original_length = len(df_clean)\n",
        "\n",
        "        # Remover duplicados MANTENIENDO el seguimiento de √≠ndices\n",
        "        df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        relevant_columns = [\n",
        "            'SEXO', 'PESO (Gramos)', 'TALLA (CentImetros)', 'TIEMPO DE GESTACION',\n",
        "            'NUMERO CONSULTAS PRENATALES', 'TIPO PARTO', 'MULTIPLICIDAD EMBARAZO',\n",
        "            'APGAR1', 'EDAD MADRE', 'NUMERO HIJOS NACIDOS VIVOS', 'NUMERO EMBARAZOS'\n",
        "        ]\n",
        "        existing_columns = [col for col in relevant_columns if col in df_clean.columns]\n",
        "        df_clean = df_clean[existing_columns].copy()\n",
        "\n",
        "        # Si existe APGAR2, remover filas donde sea null\n",
        "        if 'APGAR2' in X.columns:\n",
        "            df_clean = df_clean.dropna(subset=['APGAR2']).reset_index(drop=True)\n",
        "\n",
        "        # Remover filas con valores cr√≠ticos faltantes\n",
        "        critical_cols = ['APGAR1', 'TIEMPO DE GESTACION']\n",
        "        existing_critical = [col for col in critical_cols if col in df_clean.columns]\n",
        "        if existing_critical:\n",
        "            df_clean = df_clean.dropna(subset=existing_critical, how='any').reset_index(drop=True)\n",
        "\n",
        "        # Guardar los √≠ndices v√°lidos (ya reseteados)\n",
        "        self.valid_indices_ = df_clean.index.copy()\n",
        "\n",
        "        # NORMALIZAR TIPO PARTO\n",
        "        df_clean = self._normalizar_tipo_parto(df_clean)\n",
        "\n",
        "        # Procesar SEXO\n",
        "        if 'SEXO' in df_clean.columns:\n",
        "            df_clean['SEXO'] = df_clean['SEXO'].replace('INDETERMINADO', np.nan)\n",
        "            if df_clean['SEXO'].isnull().any():\n",
        "                self.sexo_moda_ = df_clean['SEXO'].mode()[0] if len(df_clean['SEXO'].mode()) > 0 else 'FEMENINO'\n",
        "\n",
        "        # Configurar imputador\n",
        "        imputation_cols = ['TIEMPO DE GESTACION', 'PESO (Gramos)', 'TALLA (CentImetros)']\n",
        "        existing_imputation_cols = [col for col in imputation_cols if col in df_clean.columns]\n",
        "\n",
        "        if existing_imputation_cols:\n",
        "            self.imputer_ = KNNImputer(n_neighbors=5)\n",
        "            if df_clean[existing_imputation_cols].isnull().any().any():\n",
        "                self.imputer_.fit(df_clean[existing_imputation_cols])\n",
        "\n",
        "        print(f\"Fit completado: {original_length} ‚Üí {len(df_clean)} filas\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # CLAVE: Resetear √≠ndices desde el inicio\n",
        "        df_clean = X.copy().reset_index(drop=True)\n",
        "        original_length = len(df_clean)\n",
        "\n",
        "        # IMPORTANTE: NO remover duplicados en transform para preservar longitud\n",
        "        # Solo limpiar datos sin cambiar el n√∫mero de filas\n",
        "        # df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        relevant_columns = [\n",
        "            'SEXO', 'PESO (Gramos)', 'TALLA (CentImetros)', 'TIEMPO DE GESTACION',\n",
        "            'NUMERO CONSULTAS PRENATALES', 'TIPO PARTO', 'MULTIPLICIDAD EMBARAZO',\n",
        "            'APGAR1', 'EDAD MADRE', 'NUMERO HIJOS NACIDOS VIVOS', 'NUMERO EMBARAZOS'\n",
        "        ]\n",
        "        existing_columns = [col for col in relevant_columns if col in df_clean.columns]\n",
        "        df_clean = df_clean[existing_columns].copy()\n",
        "\n",
        "        # NORMALIZAR TIPO PARTO\n",
        "        df_clean = self._normalizar_tipo_parto(df_clean)\n",
        "\n",
        "        # Procesar SEXO\n",
        "        if 'SEXO' in df_clean.columns:\n",
        "            df_clean['SEXO'] = df_clean['SEXO'].replace('INDETERMINADO', np.nan)\n",
        "            if hasattr(self, 'sexo_moda_') and self.sexo_moda_ is not None:\n",
        "                df_clean['SEXO'] = df_clean['SEXO'].fillna(self.sexo_moda_)\n",
        "            df_clean['SEXO'] = df_clean['SEXO'].fillna('FEMENINO')\n",
        "\n",
        "        # Imputaci√≥n con KNN\n",
        "        imputation_cols = ['TIEMPO DE GESTACION', 'PESO (Gramos)', 'TALLA (CentImetros)']\n",
        "        existing_imputation_cols = [col for col in imputation_cols if col in df_clean.columns]\n",
        "\n",
        "        if existing_imputation_cols and hasattr(self, 'imputer_') and self.imputer_ is not None:\n",
        "            if df_clean[existing_imputation_cols].isnull().any().any():\n",
        "                try:\n",
        "                    imputed_values = self.imputer_.transform(df_clean[existing_imputation_cols])\n",
        "                    df_clean[existing_imputation_cols] = imputed_values\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: KNN imputation failed: {e}\")\n",
        "                    # Fallback a mediana/defaults\n",
        "                    pass\n",
        "\n",
        "            # Fallback para valores a√∫n faltantes\n",
        "            for col in existing_imputation_cols:\n",
        "                if df_clean[col].isnull().any():\n",
        "                    defaults = {\n",
        "                        'TIEMPO DE GESTACION': 38,\n",
        "                        'PESO (Gramos)': 3200,\n",
        "                        'TALLA (CentImetros)': 50\n",
        "                    }\n",
        "                    median_val = df_clean[col].median()\n",
        "                    if pd.isna(median_val):\n",
        "                        median_val = defaults.get(col, 0)\n",
        "                    df_clean[col] = df_clean[col].fillna(median_val)\n",
        "\n",
        "        # Valores por defecto para columnas num√©ricas\n",
        "        numeric_defaults = {\n",
        "            'NUMERO CONSULTAS PRENATALES': 6,\n",
        "            'APGAR1': 8,\n",
        "            'EDAD MADRE': 25,\n",
        "            'NUMERO HIJOS NACIDOS VIVOS': 1,\n",
        "            'NUMERO EMBARAZOS': 1\n",
        "        }\n",
        "\n",
        "        for col, default_val in numeric_defaults.items():\n",
        "            if col in df_clean.columns and df_clean[col].isnull().any():\n",
        "                df_clean[col] = df_clean[col].fillna(default_val)\n",
        "\n",
        "        # Valores por defecto para columnas categ√≥ricas\n",
        "        categorical_defaults = {\n",
        "            'TIPO PARTO': 'ESPONTANEO',\n",
        "            'MULTIPLICIDAD EMBARAZO': 'SIMPLE'\n",
        "        }\n",
        "\n",
        "        for col, default_val in categorical_defaults.items():\n",
        "            if col in df_clean.columns and df_clean[col].isnull().any():\n",
        "                df_clean[col] = df_clean[col].fillna(default_val)\n",
        "\n",
        "        # Convertir a categ√≥ricas\n",
        "        categorical_cols = ['SEXO', 'TIPO PARTO', 'MULTIPLICIDAD EMBARAZO']\n",
        "        for col in categorical_cols:\n",
        "            if col in df_clean.columns:\n",
        "                df_clean[col] = df_clean[col].astype('category')\n",
        "\n",
        "        # One-hot encoding\n",
        "        if 'TIPO PARTO' in df_clean.columns:\n",
        "            df_clean = pd.get_dummies(df_clean, columns=['TIPO PARTO'], drop_first=False)\n",
        "\n",
        "        if 'MULTIPLICIDAD EMBARAZO' in df_clean.columns:\n",
        "            df_clean = pd.get_dummies(df_clean, columns=['MULTIPLICIDAD EMBARAZO'], drop_first=False)\n",
        "\n",
        "        if 'SEXO' in df_clean.columns:\n",
        "            df_clean = pd.get_dummies(df_clean, columns=['SEXO'], drop_first=True)\n",
        "\n",
        "        # Crear √≠ndice de masa neonatal\n",
        "        if 'PESO (Gramos)' in df_clean.columns and 'TALLA (CentImetros)' in df_clean.columns:\n",
        "            # Evitar divisi√≥n por cero\n",
        "            df_clean['TALLA (CentImetros)'] = df_clean['TALLA (CentImetros)'].replace(0, np.nan)\n",
        "            df_clean['TALLA (CentImetros)'] = df_clean['TALLA (CentImetros)'].fillna(50)\n",
        "\n",
        "            df_clean['INDICE_MASA_NEONATAL'] = df_clean['PESO (Gramos)'] / (df_clean['TALLA (CentImetros)'] ** 2)\n",
        "            df_clean['INDICE_MASA_NEONATAL'] = df_clean['INDICE_MASA_NEONATAL'].replace([np.inf, -np.inf], np.nan)\n",
        "            df_clean['INDICE_MASA_NEONATAL'] = df_clean['INDICE_MASA_NEONATAL'].fillna(1.28)\n",
        "\n",
        "        # Eliminar columnas innecesarias\n",
        "        cols_to_drop = [\n",
        "            'PESO (Gramos)', 'TALLA (CentImetros)', 'EDAD MADRE',\n",
        "            'NUMERO HIJOS NACIDOS VIVOS', 'NUMERO EMBARAZOS'\n",
        "        ]\n",
        "\n",
        "        dummy_cols_to_drop = [\n",
        "            'TIPO PARTO_INSTRUMENTADO', 'MULTIPLICIDAD EMBARAZO_DOBLE',\n",
        "            'MULTIPLICIDAD EMBARAZO_SIMPLE', 'MULTIPLICIDAD EMBARAZO_TRIPLE',\n",
        "            'SEXO_MASCULINO'\n",
        "        ]\n",
        "\n",
        "        all_cols_to_drop = cols_to_drop + dummy_cols_to_drop\n",
        "        existing_cols_to_drop = [col for col in all_cols_to_drop if col in df_clean.columns]\n",
        "\n",
        "        if existing_cols_to_drop:\n",
        "            df_clean = df_clean.drop(columns=existing_cols_to_drop)\n",
        "\n",
        "        # Llenar cualquier valor faltante restante\n",
        "        df_clean = df_clean.fillna(0)\n",
        "\n",
        "        # VERIFICACI√ìN FINAL - debe mantener el mismo n√∫mero de filas\n",
        "        final_length = len(df_clean)\n",
        "        if final_length != original_length:\n",
        "            print(f\"‚ö†Ô∏è  ADVERTENCIA: Cambio en n√∫mero de filas: {original_length} ‚Üí {final_length}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Filas preservadas: {original_length}\")\n",
        "\n",
        "        # Asegurar que el DataFrame tenga √≠ndices consecutivos\n",
        "        df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "        return df_clean\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def cargar_modelo(path='/content/pipeline_apgar_streamlit.pkl'):\n",
        "    return joblib.load(path)\n",
        "\n",
        "pipeline = cargar_modelo()\n",
        "\n",
        "archivo_csv = st.file_uploader(\"Sube un archivo CSV\", type=['csv'])\n",
        "\n",
        "if archivo_csv is not None:\n",
        "    try:\n",
        "        datos = pd.read_csv(archivo_csv)\n",
        "        st.write(\"Datos cargados exitosamente:\")\n",
        "        st.dataframe(datos.head())\n",
        "\n",
        "        st.subheader(\"Predicci√≥n de APGAR2\")\n",
        "        predicciones = pipeline.predict(datos)\n",
        "        datos['APGAR2_PREDICTED'] = predicciones\n",
        "\n",
        "        st.write(\"Resultados:\")\n",
        "        st.dataframe(datos[['APGAR2_PREDICTED']].head())\n",
        "\n",
        "        st.download_button(\n",
        "            label=\"Descargar resultados\",\n",
        "            data=datos.to_csv(index=False).encode('utf-8'),\n",
        "            file_name=\"predicciones_apgar2.csv\",\n",
        "            mime='text/csv'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error al procesar el archivo: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Correr la app en background, recuerda cargar el archivo app.py\n",
        "!nohup streamlit run app.py &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-JsIuAYyHze",
        "outputId": "0faa2d1a-08e4-44c8-813b-105551b8b09d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKZtT5KRySB1",
        "outputId": "a6ebf9b5-3598-4113-d2e8-0b57887bb065"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 1s\n",
            "\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#La IP de salida es la clave que el local tunel necesita para ejecutar\n",
        "import urllib.request\n",
        "\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdqssWRWyawV",
        "outputId": "c40b57c0-3310-4b8b-f427-3d96c326e36f"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.125.69.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se crea un t√∫nel hacia un servidor local que se est√° ejecutando en el puerto 8501.\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtJgWEWcyjHF",
        "outputId": "f2ab5ac2-de23-4e15-8eff-716874dc231c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0Kyour url is: https://neat-bottles-boil.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}